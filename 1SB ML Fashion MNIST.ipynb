{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"text-align: center;\">Data Visualization Project</h1>\n",
    "<h2 style=\"text-align: center;\">Fashion-MNIST Dataset</h2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preface\n",
    "* Fashion-MNIST is a dataset containing 70,000 samples, 60,000 for the training set and 10,000 for the test set.\n",
    "* Each sample is a 28x28 (784 pixels) grayscale image of a certain fashion item.\n",
    "* The data contains a column with 10 labels, making this a **multiclass classification** problem.\n",
    "In other words, this is a **supervised learning** task.\n",
    "* The model will be trained using all available data and run without learning anymore, also known as **offline/batch learning**.\n",
    "* ***Main objective:*** Find the best algorithm and model parameters that classify the unseen images correctly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier, cv, DMatrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import clone  # 'clone' constructs a new unfitted estimator with the same parameters\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "# global variables\n",
    "label_dict = {0: \"0 = T-shirt/top\", 1: \"1 = Trouser\", 2: \"2 = Pullover\", 3: \"3 = Dress\", 4: \"4 = Coat\",\n",
    "              5: \"5 = Sandal\", 6: \"6 = Shirt\", 7: \"7 = Sneaker\", 8: \"8 = Bag\", 9: \"9 = Ankle boot\"}\n",
    "\n",
    "# display setup\n",
    "plt.style.use('seaborn')  # for plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Getting the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "train_set = pd.read_csv(r\"FMNIST/fashion-mnist_train.csv\")\n",
    "test_set = pd.read_csv(r\"FMNIST/fashion-mnist_test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the first 5 rows for a quick look\n",
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the last 5 rows for a quick look\n",
    "train_set.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DataFrame shape (rows, columns)\n",
    "print(\"Training Set:\", train_set.shape)\n",
    "print(\"Test Set:\", test_set.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# description of data\n",
    "train_set.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# summary of the numerical attributes\n",
    "train_set.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# maximum pixel value\n",
    "train_set.describe().loc['max'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# minimum pixel value\n",
    "train_set.describe().loc['min'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Features in the DataFrame:\n",
    "> There are 785 columns, one for the labels and 784 for the pixels (one for each pixel).\n",
    ">> Labels:\n",
    "> - 0 = T-shirt/top\n",
    "> - 1 = Trouser\n",
    "> - 2 = Pullover\n",
    "> - 3 = Dress\n",
    "> - 4 = Coat\n",
    "> - 5 = Sandal\n",
    "> - 6 = Shirt\n",
    "> - 7 = Sneaker\n",
    "> - 8 = Bag\n",
    "> - 9 = Ankle boot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of instances for each category\n",
    "train_set[\"label\"].value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Initial observations:\n",
    "* Each category has an equal amount of samples in the training set, making this a **balanced classification** task.\n",
    "* Classes and pixel values are integers.\n",
    "* The pixel range is [0, 255]. Some feature columns have a smaller maximum value or a\n",
    "greater minimum value. This means that the values range is smaller for some pixels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Understanding and Visualizing the Data\n",
    "> ##### *The motivation for this section is to gain more insights.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The data was split in advance and the images are already the same aspect ratio.\n",
    "Let's create a copy of the data to prevent accidentally harming the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# deep copy of the training set\n",
    "fmnist = train_set.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fmnist.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "# np.isnan checks if the element is is not a number\n",
    "# df.values returns a numpy array containing the data without index or column names\n",
    "# sum() returns the absolute amount missing\n",
    "np.isnan(fmnist.values).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "> * There are no missing values in the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Exploring Color Transformations on a Sample Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample image\n",
    "some_sample = fmnist.drop('label', axis=1).iloc[0]  # get sample\n",
    "some_sample = np.array(some_sample)  # convert to array\n",
    "some_sample_img = some_sample.reshape(28, 28)  # reshape array\n",
    "\n",
    "# convert sample image from grayscale to black and white\n",
    "# in the fashion mnist dataset- white pixels are 0 and black pixels are 255\n",
    "# the images are grayscale, so values (0,255) are different intensities of gray\n",
    "some_sample_bin = some_sample.copy()  # deep copy of the sample image\n",
    "some_sample_bin[some_sample_bin > 0] = 1  # convert gray intensities to black\n",
    "some_sample_bin_img = some_sample_bin.reshape(28, 28)  # reshape array\n",
    "\n",
    "fig, dx = plt.subplots(2, 2, figsize=(12, 7))\n",
    "\n",
    "# plot grayscale sample image and pixel value occurrences\n",
    "dx[0, 0].imshow(some_sample_img)\n",
    "dx[0, 0].axis('off')\n",
    "dx[0, 0].set_title(\"Original Grayscale Sample\", size=15)\n",
    "dx[0, 1].hist(some_sample, bins=50)\n",
    "dx[0, 1].set_title(\"Grayscale Sample Pixel Value Occurrences\", size=15)\n",
    "dx[0, 1].set_xlabel(\"Pixel Value\")\n",
    "dx[0, 1].set_ylabel(\"Count\")\n",
    "\n",
    "# plot black and white sample image and pixel value occurrences\n",
    "dx[1, 0].imshow(some_sample_bin_img)\n",
    "dx[1, 0].axis('off')\n",
    "dx[1, 0].set_title(\"Black and White Sample\", size=15)\n",
    "dx[1, 1].hist(some_sample_bin, bins=50)\n",
    "dx[1, 1].set_title(\"Black and White Sample Pixel Value Occurrences\", size=15)\n",
    "dx[1, 1].set_xlabel(\"Pixel Value\")\n",
    "dx[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* The most common pixel value in the original grayscale image is white (0) and in the binary\n",
    "image is black (1 = 255 in grayscale).\n",
    "* Aside from a few pixels that are detached from the clothing, the black and white pullover resembles\n",
    "the original grayscale shape.\n",
    "* Training models on both options and comparing the results can help determine if transforming the\n",
    "images to black and white is preferable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Class Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot image for each category\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "rows = 2\n",
    "columns = 5\n",
    "# use groupby to locate an instance for each label\n",
    "label_groups = fmnist.groupby('label')\n",
    "# add image in each iteration\n",
    "for i in range(rows * columns):\n",
    "    curr = label_groups.get_group(i)[:1]  # get group\n",
    "    curr_img = curr.drop('label', axis=1).to_numpy().reshape(28, 28)  # convert to reshaped array\n",
    "    fig.add_subplot(rows, columns, i + 1)\n",
    "    plt.imshow(curr_img)\n",
    "    plt.axis('off')  # remove grid\n",
    "    plt.title(label_dict[i])  # use dictionary to add subplot title\n",
    "\n",
    "fig.suptitle(\"Fashion-MNIST Samples\", size=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot pixel value occurrences for each class\n",
    "\n",
    "fig, dx = plt.subplots(2, 5, figsize=(12, 7), sharey='all')\n",
    "i = 0  # current group label\n",
    "mean_values = []\n",
    "plt.setp(dx, xticks=np.arange(0, 256, step=85))  # set x axis values\n",
    "\n",
    "for row in range(2):\n",
    "    for col in range(5):\n",
    "        pixels = np.array(label_groups.get_group(i).drop(['label'], axis=1))  # get group and convert to array\n",
    "        mean_values.append(pixels.mean())  # calculate mean pixel value and add to list (for next plot)\n",
    "        dx[row, col].hist(pixels.reshape(-1))  # add histogram in each iteration, -1 reshapes to length of array\n",
    "        dx[row, col].set_title(label_dict[i], size=15)  # use dictionary to add subplot title\n",
    "        i = i + 1  # next group\n",
    "\n",
    "fig.suptitle(\"Pixel Occurrences per Class\", size=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot mean values calculated in previous cell\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=np.arange(10), y=mean_values)  # x axis for classes, y axis for mean values\n",
    "plt.xticks(np.arange(10), labels=label_dict.values())  # use dictionary to set x axis values\n",
    "plt.xlabel(\"Class\", size=15)\n",
    "plt.ylabel(\"Mean\", size=15)\n",
    "plt.title(\"Pixel Occurrences per Class Mean\", size=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* All classes have a majority of white pixel values (0), with the rest scattered in the remaining range.\n",
    "* Aside from the white pixel value, quite a few of the histograms displaying pixel occurrences look like\n",
    "they have a somewhat normal distribution. They are tail-heavy, extending farther to the right.\n",
    "* Any shoe type (classes 5, 7, 9) have the most white pixels, with sandal (class 5) containing the most.\n",
    "This is emphasized most in the pixel occurrences mean plot.\n",
    "* Coats (class 4) and pullovers (class 2) have the highest pixel mean. When looking at the sample\n",
    "images it is noticeable that they fill most of the diagram.\n",
    "* The t-shirt/top (class 0) and shirt (class 6) have extremely similar pixel occurrences.\n",
    "Had the class labels been removed, the graphs would be nearly indistinguishable. We can assume that\n",
    "the models will make mistakes when predicting instances from these two classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Analyze and Compare Sample Images of T-shirt/top and Shirt Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# t-shirt/top sample image (class 0)\n",
    "top = label_groups.get_group(0).drop('label', axis=1).iloc[0]  # get t-shirt/top sample\n",
    "top = np.array(top)  # convert to array\n",
    "top_img = top.reshape(28, 28)  # reshape array\n",
    "\n",
    "# convert t-shirt/top sample image from grayscale to black and white\n",
    "top_bin = top.copy()  # deep copy of the t-shirt/top sample image\n",
    "top_bin[top_bin > 0] = 1  # convert gray intensities to black\n",
    "top_bin_img = top_bin.reshape(28, 28)  # reshape array\n",
    "\n",
    "# shirt sample image (class 6)\n",
    "shirt = label_groups.get_group(6).drop('label', axis=1).iloc[0]  # get t-shirt/top sample\n",
    "shirt = np.array(shirt)  # convert to array\n",
    "shirt_img = shirt.reshape(28, 28)  # reshape array\n",
    "\n",
    "# convert shirt sample image from grayscale to black and white\n",
    "shirt_bin = shirt.copy()  # deep copy of the shirt sample image\n",
    "shirt_bin[shirt_bin > 0] = 1  # convert gray intensities to black\n",
    "shirt_bin_img = shirt_bin.reshape(28, 28)  # reshape array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, dx = plt.subplots(4, 2, figsize=(10, 10))\n",
    "\n",
    "# plot grayscale t-shirt/top sample image and pixel value occurrences\n",
    "dx[0, 0].imshow(top_img)\n",
    "dx[0, 0].axis('off')\n",
    "dx[0, 0].set_title(\"Grayscale T-shirt/top\", size=15)\n",
    "dx[0, 1].hist(top)\n",
    "dx[0, 1].set_title(\"Grayscale T-shirt/top Pixel Value Occurrences\", size=15)\n",
    "dx[0, 1].set_xlabel(\"Pixel Value\")\n",
    "dx[0, 1].set_ylabel(\"Count\")\n",
    "# plot black and white t-shirt/top sample image and pixel value occurrences\n",
    "dx[1, 0].imshow(top_bin_img)\n",
    "dx[1, 0].axis('off')\n",
    "dx[1, 0].set_title(\"Black and White T-shirt/top\", size=15)\n",
    "dx[1, 1].hist(top_bin)\n",
    "dx[1, 1].set_title(\"Black and White T-shirt/top Pixel Value Occurrences\", size=15)\n",
    "dx[1, 1].set_xticks([0, 1])\n",
    "dx[1, 1].set_xlabel(\"Pixel Value\")\n",
    "dx[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "# plot grayscale shirt sample image and pixel value occurrences\n",
    "dx[2, 0].imshow(shirt_img)\n",
    "dx[2, 0].axis('off')\n",
    "dx[2, 0].set_title(\"Grayscale Shirt\", size=15)\n",
    "dx[2, 1].hist(shirt)\n",
    "dx[2, 1].set_title(\"Grayscale Shirt Pixel Value Occurrences\", size=15)\n",
    "dx[2, 1].set_xlabel(\"Pixel Value\")\n",
    "dx[2, 1].set_ylabel(\"Count\")\n",
    "# plot black and white shirt sample image and pixel value occurrences\n",
    "dx[3, 0].imshow(shirt_bin_img)\n",
    "dx[3, 0].axis('off')\n",
    "dx[3, 0].set_title(\"Black and White Shirt\", size=15)\n",
    "dx[3, 1].hist(shirt_bin)\n",
    "dx[3, 1].set_title(\"Black and White Shirt Pixel Value Occurrences\", size=15)\n",
    "dx[3, 1].set_xticks([0, 1])\n",
    "dx[3, 1].set_xlabel(\"Pixel Value\")\n",
    "dx[3, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* The main difference between the classes are the pixels representing the sleeves.\n",
    "The t-shirt/top has short sleeves and the shirt has long sleeves.\n",
    "* Converting the images to binary shows where the mix-up between the classes might be.\n",
    "The t-shirt has gray pixels that aren't seen in the grayscale image, but stand out in the\n",
    "black and white image. The unseen gray pixels appear where the long sleeves would be-\n",
    "had this been a shirt.\n",
    "* The pixel value occurrences are similar when comparing the black and white images and are\n",
    "spread out differently in the grayscale images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean copy of the training set\n",
    "df = train_set.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# separate features from target values\n",
    "\n",
    "# drop- creates a copy without changing the training set\n",
    "X_train = df.drop('label', axis=1)\n",
    "\n",
    "# create a deep copy of the target values\n",
    "y_train = df['label'].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Custom Transformer:\n",
    "> In section 2, I evaluated samples in grayscale and in black and white.\n",
    "> I assumed that converting images to black and white is a worthwhile transformation to look into\n",
    "> and could improve the ML algorithms.\n",
    ">\n",
    "> The following custom transformer automates this process on the entire dataset:\n",
    ">\n",
    ">> The ColorConverter transformer contains the hyperparameter \"is_binary\". It is\n",
    "> set by default to 'False', meaning the samples remain in the original grayscale format.\n",
    "> However, the hyperparameter can be set to 'True' which will convert the images to black\n",
    "> and white.\n",
    ">>\n",
    ">> Adding this transformer will allow to easily switch between the two options during\n",
    "> model training, and determine which one is better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# custom transformer for converting images to black and white\n",
    "\n",
    "# BaseEstimator for enabling hyperparameters\n",
    "# TransformerMixin adds fit_transform method\n",
    "class ColorConverter(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, is_binary=False):\n",
    "        self.is_binary = is_binary\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.is_binary:\n",
    "            X_binary = X.copy() # deep copy to avoid harming the original data\n",
    "            X_binary[X_binary > 0] = 1 # convert grayscale intensities to black\n",
    "            return X_binary\n",
    "        else:\n",
    "            return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Feature Scaling\n",
    ">\n",
    "> Although the pixel values are in a known range [0,255], scaling the data can make a crucial difference\n",
    "(especially if the learning algorithm relies on calculating distances).\n",
    ">\n",
    ">> Why is this important?\n",
    "* Models can't differentiate feature importance the same way humans can.\n",
    "A training algorithm may assume that a feature containing large numbers is more important than features\n",
    "with smaller numbers- which might not be the case.\n",
    "* Some algorithms converge much faster when features are scaled (i.e. Gradient Descent).\n",
    "* There are ML algorithms that make assumptions on the data (i.e. PCA assumes the data is centered around\n",
    "the origin).\n",
    "\n",
    "\n",
    "> Chosen feature scale:\n",
    ">\n",
    "> Standardizing centers the data so that it has a zero mean and a standard deviation of 1, under the assumption\n",
    "> that the data is normally distributed.\n",
    ">\n",
    "> * The distribution is relatively normal (aside from the white pixels which is highest in all classes).\n",
    "* Using PCA could be useful since the dataset has a large amount of features. As previously stated,\n",
    "PCA assumes the data has zero mean.\n",
    ">\n",
    "> Therefore, standard scaling is the ideal option."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create transformation pipeline\n",
    "\n",
    "# How to change ColorConverter is_binary hyperparameter:\n",
    "# full_pipeline['clr_convert'].__setattr__('is_binary', True)\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('clr_convert', ColorConverter()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform training data using pipeline\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Training and Evaluating Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> As previously mentioned, the number of instances for each class in the training set are equal.\n",
    ">\n",
    "> Chosen evaluation metric:\n",
    ">\n",
    "> Accuracy works well with balanced classification tasks. It is also the most intuitive\n",
    "> metric and is especially easier to understand when dealing with multiclass classification.\n",
    ">> Note:\n",
    "* Accuracy is sensitive to the test size. Therefore, I will use 6 cross-validation folds\n",
    "(total of 10,000 instances in each test fold- matching the test set size).\n",
    "* Since the features consist of pixel values (a fixed range), it is likely that the testing data would fit the\n",
    "transformers nearly the same. This means that cross-validation should give good assessment\n",
    "of the performance on unseen data, even when evaluating only from the fitted and transformed training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function for evaluating cross validation scores and for easy model comparison\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", round(scores.mean(), 5))\n",
    "    print(\"Standard Deviation:\", round(scores.std(), 5))\n",
    "\n",
    "# function prints accuracy and errors\n",
    "def display_evaluation(actual, pred, print_report=False, print_errors=False):\n",
    "    print(\"Accuracy:\", round(metrics.accuracy_score(actual, pred), 5))\n",
    "    if print_report:\n",
    "        print(metrics.classification_report(actual, pred))\n",
    "    if print_errors:\n",
    "        print(\"Confusion Matrix Errors:\")\n",
    "        conf_mx = metrics.confusion_matrix(actual, pred, labels=range(10))\n",
    "        # confusion matrix rows represent actual class, columns represent predicted class\n",
    "        row_sums = conf_mx.sum(axis=1, keepdims=True) # count sum of instances in each row (per class)\n",
    "        norm_conf_mx = conf_mx / row_sums # divide confusion matrix by number of instances in each class\n",
    "        np.fill_diagonal(norm_conf_mx, 0) # keep errors only by filling diagonal\n",
    "        plt.matshow(norm_conf_mx, cmap=plt.cm.gray) # plot errors in confusion matrix\n",
    "        plt.grid(False)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shortlist Promising Models:\n",
    "> Trying many models quickly and selecting the ones that show promising results.\n",
    ">\n",
    ">> How I plan to do this:\n",
    "1. Train a baseline model and evaluate sample predictions.\n",
    "2. Use cross-validation and evaluate the scores.\n",
    "3. If the model has a significant hyperparameter, try changing it. Use cross-validation to\n",
    "compare the results to step 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a few instances from the training data for testing\n",
    "some_data = X_train.iloc[:10]\n",
    "some_labels = y_train.iloc[:10]\n",
    "some_data_prepared = full_pipeline.transform(some_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dummy Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=42)  # stratified uses training set class distribution\n",
    "dummy_clf.fit(X_train_prepared, y_train)\n",
    "dummy_clf_pred = cross_val_predict(dummy_clf, X_train_prepared, y_train, cv=6)\n",
    "display_evaluation(y_train, dummy_clf_pred, True, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Logistic Regression: Grayscale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# multinomial = softmax regression, used for multiclass\n",
    "# softmax normalizes the total probability to a sum of 1\n",
    "log_reg = LogisticRegression(multi_class='multinomial', random_state=42, n_jobs=-1)\n",
    "# log_reg.fit(X_train_prepared, y_train)\n",
    "# joblib.dump(log_reg, \"FMNIST/models/log_reg_1.pkl\")\n",
    "log_reg = joblib.load(r\"FMNIST/models/log_reg_1.pkl\")\n",
    "\n",
    "print(\"Predictions:\", log_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# log_reg_scores_1 = cross_val_score(log_reg, X_train_prepared, y_train, scoring='accuracy', cv=6)\n",
    "# joblib.dump(log_reg_scores_1, \"FMNIST/scores/log_reg_scores_1.pkl\")\n",
    "log_reg_scores_1 = joblib.load(r\"FMNIST/scores/log_reg_scores_1.pkl\")\n",
    "display_scores(log_reg_scores_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. KNN: Grayscale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using default weights (uniform)\n",
    "# using default n_neighbors (n = 5)\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "# knn_clf.fit(X_train_prepared, y_train)\n",
    "# joblib.dump(knn_clf, \"FMNIST/models/knn_clf_2.pkl\")\n",
    "knn_clf = joblib.load(r\"FMNIST/models/knn_clf_2.pkl\")\n",
    "\n",
    "print(\"Predictions:\", knn_clf.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# knn_scores_2 = cross_val_score(knn_clf, X_train_prepared, y_train, scoring='accuracy', cv=6)\n",
    "# joblib.dump(knn_scores_2, \"FMNIST/scores/knn_scores_2.pkl\")\n",
    "knn_scores_2 = joblib.load(r\"FMNIST/scores/knn_scores_2.pkl\")\n",
    "display_scores(knn_scores_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Decision Tree Classifier: Grayscale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "# tree_clf.fit(X_train_prepared, y_train)\n",
    "# joblib.dump(tree_clf, \"FMNIST/models/tree_clf_3.pkl\")\n",
    "tree_clf = joblib.load(r\"FMNIST/models/tree_clf_3.pkl\")\n",
    "\n",
    "print(\"Predictions:\", tree_clf.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tree_scores_3 = cross_val_score(tree_clf, X_train_prepared, y_train, scoring='accuracy', cv=6, n_jobs=-1)\n",
    "# joblib.dump(tree_scores_3, \"FMNIST/scores/tree_scores_3.pkl\")\n",
    "tree_scores_3 = joblib.load(r\"FMNIST/scores/tree_scores_3.pkl\")\n",
    "display_scores(tree_scores_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Random Forest Classifier: Grayscale"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "# rf_clf.fit(X_train_prepared, y_train)\n",
    "# joblib.dump(rf_clf, \"FMNIST/models/rf_clf_4.pkl\")\n",
    "rf_clf = joblib.load(r\"FMNIST/models/rf_clf_4.pkl\")\n",
    "\n",
    "print(\"Predictions:\", rf_clf.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rf_scores_4 = cross_val_score(rf_clf, X_train_prepared, y_train, scoring='accuracy', cv=6)\n",
    "# joblib.dump(rf_scores_4, \"FMNIST/scores/rf_scores_4.pkl\")\n",
    "rf_scores_4 = joblib.load(r\"FMNIST/scores/rf_scores_4.pkl\")\n",
    "display_scores(rf_scores_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ***Before continuing, I will try these 4 models again on the black and white dataset.***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# change hyperparameter to black and white images\n",
    "full_pipeline['clr_convert'].__setattr__('is_binary', True)\n",
    "full_pipeline.steps[0] # check that the hyperparameter changed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform training data using pipeline\n",
    "X_train_prepared2 = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "# a few instances from the training data for testing\n",
    "some_data_prepared2 = full_pipeline.transform(some_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Note: Fitting the same model will overwrite the previous fit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Logistic Regression: Black and White"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# log_reg.fit(X_train_prepared2, y_train)\n",
    "# joblib.dump(log_reg, \"FMNIST/models/log_reg_5.pkl\")\n",
    "log_reg = joblib.load(r\"FMNIST/models/log_reg_5.pkl\")\n",
    "\n",
    "print(\"Predictions:\", log_reg.predict(some_data_prepared2))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# log_reg_scores_5 = cross_val_score(log_reg, X_train_prepared2, y_train, scoring='accuracy', cv=6)\n",
    "# joblib.dump(log_reg_scores_5, \"FMNIST/scores/log_reg_scores_5.pkl\")\n",
    "log_reg_scores_5 = joblib.load(r\"FMNIST/scores/log_reg_scores_5.pkl\")\n",
    "display_scores(log_reg_scores_5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. KNN: Black and White"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# knn_clf.fit(X_train_prepared2, y_train)\n",
    "# joblib.dump(knn_clf, \"FMNIST/models/knn_clf_6.pkl\")\n",
    "knn_clf = joblib.load(r\"FMNIST/models/knn_clf_6.pkl\")\n",
    "\n",
    "print(\"Predictions:\", knn_clf.predict(some_data_prepared2))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# knn_scores_6 = cross_val_score(knn_clf, X_train_prepared2, y_train, scoring='accuracy', cv=6)\n",
    "# joblib.dump(knn_scores_6, \"FMNIST/scores/knn_scores_6.pkl\")\n",
    "knn_scores_6 = joblib.load(r\"FMNIST/scores/knn_scores_6.pkl\")\n",
    "display_scores(knn_scores_6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Decision Tree Classifier: Black and White"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tree_clf.fit(X_train_prepared2, y_train)\n",
    "# joblib.dump(tree_clf, \"FMNIST/models/tree_clf_7.pkl\")\n",
    "tree_clf = joblib.load(r\"FMNIST/models/tree_clf_7.pkl\")\n",
    "\n",
    "print(\"Predictions:\", tree_clf.predict(some_data_prepared2))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tree_scores_7 = cross_val_score(tree_clf, X_train_prepared2, y_train, scoring='accuracy', cv=6, n_jobs=-1)\n",
    "# joblib.dump(tree_scores_7, \"FMNIST/scores/tree_scores_7.pkl\")\n",
    "tree_scores_7 = joblib.load(r\"FMNIST/scores/tree_scores_7.pkl\")\n",
    "display_scores(tree_scores_7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. Random Forest Classifier: Black and White"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rf_clf.fit(X_train_prepared2, y_train)\n",
    "# joblib.dump(rf_clf, \"FMNIST/models/rf_clf_8.pkl\")\n",
    "rf_clf = joblib.load(r\"FMNIST/models/rf_clf_8.pkl\")\n",
    "\n",
    "print(\"Predictions:\", rf_clf.predict(some_data_prepared2))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rf_scores_8 = cross_val_score(rf_clf, X_train_prepared2, y_train, scoring='accuracy', cv=6)\n",
    "# joblib.dump(rf_scores_8, \"FMNIST/scores/rf_scores_8.pkl\")\n",
    "rf_scores_8 = joblib.load(r\"FMNIST/scores/rf_scores_8.pkl\")\n",
    "display_scores(rf_scores_8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* All models performed slightly worse after converting the images to black and white, the mean after cross-validation\n",
    "was lower."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### PCA\n",
    ">\n",
    "> PCA is a dimensionality reduction technique that projects the data onto a lower-dimensional hyperplane.\n",
    "> The ideal projection is one that preserves most of the variance with the least amount of\n",
    "> information loss. There are various ways to select the number of principal components/dimensions,\n",
    "> such as plotting the explained variance or measuring the performance of a Machine Learning\n",
    "> algorithm if PCA is used for pre-processing.\n",
    ">\n",
    "> Evaluating the models on both grayscale and black and white images was a vital step. While PCA does improve\n",
    "> training computation time, some data is lost, so it is better to choose the transformation that has better results.\n",
    ">\n",
    "> Therefore, I will use the grayscale images for PCA.\n",
    ">\n",
    ">> #### PCA vs. K-Means:\n",
    "> PCA works well with continuous values such as image pixels.\n",
    "> Furthermore, K-Means clustering is an unsupervised learning technique which outputs new labels according to\n",
    "> the assigned clusters. Since the data labels are specific (i.e. the clothing could have been split into\n",
    "> 5 categories instead of 10: tops, bottoms, dresses, footwear, bags), clustering does not seem to be the best approach\n",
    "> in this case."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# change hyperparameter to grayscale images\n",
    "full_pipeline['clr_convert'].__setattr__('is_binary', False)\n",
    "full_pipeline.steps[0]  # check that the hyperparameter changed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_prepared)\n",
    "# cumulative sum (increasing by sequential addition) of components\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.axis([0, 784, 0, 1])  # axis limits\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.xlabel(\"Dimensions\", size=15)\n",
    "plt.ylabel(\"Explained Variance\", size=15)\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Choosing the Number of Dimensions:\n",
    "> Using the elbow method, around 95% explained variance is where the graph starts growing slower.\n",
    "> For instance, reducing the dimensions from 784 down to 256 will preserve 95% of the variance with only\n",
    "> a little data loss.\n",
    ">\n",
    "> I will use pipelines and cross-validation to determine the optimal explained variance value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10. Logistic Regression: Grayscale, PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Before PCA:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_scores(log_reg_scores_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> After PCA:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_log_reg = Pipeline([\n",
    "    (\"pca\", PCA()),\n",
    "    (\"log_reg\", LogisticRegression(multi_class='multinomial', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"pca__n_components\": [0.9, 0.91, 0.92, 0.93, 0.94, 0.95]\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pca_log_reg_grid = GridSearchCV(pca_log_reg, param_grid, cv=6, scoring=\"accuracy\", verbose=2, n_jobs=-1)\n",
    "# pca_log_reg_grid.fit(X_train_prepared, y_train)\n",
    "# joblib.dump(pca_log_reg_grid, \"FMNIST/scores/pca_log_reg_grid.pkl\")\n",
    "pca_log_reg_grid = joblib.load(r\"FMNIST/scores/pca_log_reg_grid.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_log_reg_grid.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_log_reg_grid.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = pca_log_reg_grid.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 11. Random Forest: Grayscale, PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Random Forest before PCA:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_scores(rf_scores_4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Random Forest after PCA:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_rf_clf = Pipeline([\n",
    "    (\"pca\", PCA()),\n",
    "    (\"rf_clf\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pca_rf_clf_grid = GridSearchCV(pca_rf_clf, param_grid, cv=6, scoring=\"accuracy\", verbose=2)\n",
    "# pca_rf_clf_grid.fit(X_train_prepared, y_train)\n",
    "# joblib.dump(pca_rf_clf_grid, \"FMNIST/scores/pca_rf_clf_grid.pkl\")\n",
    "pca_rf_clf_grid = joblib.load(r\"FMNIST/scores/pca_rf_clf_grid.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_rf_clf_grid.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_rf_clf_grid.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = pca_rf_clf_grid.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* Logistic Regression had the highest score with 0.95 explained variance, slightly higher than before PCA.\n",
    "* Random Forest had the highest score with 0.92 explained variance, the score is roughly 2% lower than before PCA.\n",
    "* The Random Forest scores were all nearly the same. Meanwhile, the Logistic Regression scores show a downward trend.\n",
    "Choosing 0.91 explained variance seems like a reasonable ratio, preserving most of the data and model accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = np.argmax(cumsum >= 0.91) + 1\n",
    "d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> PCA reduces the dimensions from 784 to 153!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add PCA to pipeline\n",
    "full_pipeline.steps.append(('pca', PCA(n_components=0.91)))\n",
    "full_pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform data with reduced dimensions\n",
    "X_train_reduced = full_pipeline.fit_transform(X_train, y_train)\n",
    "some_data_reduced = full_pipeline.transform(some_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12. KNN: Grayscale, PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# knn_scores_12 = cross_val_score(clone(knn_clf), X_train_reduced, y_train, cv=6, scoring='accuracy', verbose=2)\n",
    "# joblib.dump(knn_scores_12, \"FMNIST/scores/knn_scores_12.pkl\")\n",
    "knn_scores_12 = joblib.load(r\"FMNIST/scores/knn_scores_12.pkl\")\n",
    "display_scores(knn_scores_12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 13. Decision Tree: Grayscale, PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tree_scores_13 = cross_val_score(clone(tree_clf), X_train_reduced, y_train, cv=6, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "# joblib.dump(tree_scores_13, \"FMNIST/scores/tree_scores_13.pkl\")\n",
    "tree_scores_13 = joblib.load(r\"FMNIST/scores/tree_scores_13.pkl\")\n",
    "display_scores(tree_scores_13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 14. Extra Trees Classifier: Grayscale, PCA\n",
    "> Very similar to the Random Forest classifier. The main difference is that Extra Trees randomly splits\n",
    "> nodes, while Random Forest searches for the best split. Extra Trees is also faster to train due to this.\n",
    ">> Note: Comparing prediction accuracy can help determine which model performs better on the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex_trees = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n",
    "# ex_trees.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(ex_trees, \"FMNIST/models/ex_trees.pkl\")\n",
    "ex_trees = joblib.load(r\"FMNIST/models/ex_trees.pkl\")\n",
    "\n",
    "print(\"Predictions:\", ex_trees.predict(some_data_reduced))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ex_trees_scores_14 = cross_val_score(ex_trees, X_train_reduced, y_train, cv=6, scoring='accuracy', verbose=2)\n",
    "# joblib.dump(ex_trees_scores_14, \"FMNIST/scores/ex_trees_scores_14.pkl\")\n",
    "ex_trees_scores_14 = joblib.load(r\"FMNIST/scores/ex_trees_scores_14.pkl\")\n",
    "display_scores(ex_trees_scores_14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Comparing Extra Trees to Random Forest:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rf_clf_scores_14 = cross_val_score(clone(rf_clf), X_train_reduced, y_train, cv=6, scoring='accuracy', verbose=2)\n",
    "# joblib.dump(rf_clf_scores_14, \"FMNIST/scores/rf_clf_scores_14.pkl\")\n",
    "rf_clf_scores_14 = joblib.load(r\"FMNIST/scores/rf_clf_scores_14.pkl\")\n",
    "display_scores(rf_clf_scores_14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* Extra Trees has a higher accuracy and a lower standard deviation, making it more reliable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 15. AdaBoost: Grayscale, PCA\n",
    "> The AdaBoost (adaptive boosting) ensemble method trains a weak classifier, then increases the\n",
    "> weights of the misclassified instances and trains another weak classifier on the dataset with the\n",
    "> updated weights.\n",
    ">> Note: The sklearn default algorithm (\"SAMME.R\") is multiclass. SAMME.R uses class probabilities\n",
    ">> and SAMME uses the predicted classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(), random_state=42)\n",
    "# ada_clf.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(ada_clf, \"FMNIST/models/ada_clf_15.pkl\")\n",
    "ada_clf = joblib.load(r\"FMNIST/models/ada_clf_15.pkl\")\n",
    "\n",
    "print(\"Predictions:\", ada_clf.predict(some_data_reduced))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ada_scores_15 = cross_val_score(ada_clf, X_train_reduced, y_train, cv=6, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "# joblib.dump(ada_scores_15, \"FMNIST/scores/ada_scores_15.pkl\")\n",
    "ada_scores_15 = joblib.load(r\"FMNIST/scores/ada_scores_15.pkl\")\n",
    "display_scores(ada_scores_15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 16. XGBoost: Grayscale, PCA\n",
    "> The Gradient Boosting ensemble method adds predictors by fitting a weak classifier to the errors\n",
    "> made by the previous classifiers, then making predictions on the combined classifiers.\n",
    ">> Note: XGBoost (extreme gradient boosting) is a separate library from sklearn\n",
    "> that contains an optimized version of gradient boosting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_clf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='merror', objective='multi:softmax',\n",
    "#                        num_class=10, use_label_encoder=False)\n",
    "# xgb_clf.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(xgb_clf, \"FMNIST/models/xgb_clf_16.pkl\")\n",
    "xgb_clf = joblib.load(r\"FMNIST/models/xgb_clf_16.pkl\")\n",
    "\n",
    "print(\"Predictions:\", xgb_clf.predict(some_data_reduced))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_clf.best_iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 10,\n",
    "    \"eval_metric\": \"merror\"  # merror rate = 1-accuracy, m stands for multiclass\n",
    "}\n",
    "\n",
    "# dmatrix is a data structure used by the XGBoost library that optimizes performance\n",
    "# it is needed when using the XGBoost library cross-validation\n",
    "dmat_train = DMatrix(X_train_reduced, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_scores_16 = cv(params, dmat_train, num_boost_round=100, nfold=6,\n",
    "#                   early_stopping_rounds=5, seed=42, metrics=\"merror\", verbose_eval=True)\n",
    "# joblib.dump(xgb_scores_16, \"FMNIST/scores/xgb_scores_16.pkl\")\n",
    "xgb_scores_16 = joblib.load(r\"FMNIST/scores/xgb_scores_16.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "curr_min = xgb_scores_16[\"test-merror-mean\"].argmin()\n",
    "curr_min"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot classification errors from cross-validation\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.lineplot(data=[xgb_scores_16[\"test-merror-mean\"], xgb_scores_16[\"train-merror-mean\"]])\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel(\"classification error rate\")\n",
    "plt.title(\"XGBoost Classification Errors\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best accuracy\n",
    "1 - xgb_scores_16[\"test-merror-mean\"][curr_min]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* So far, the minimum errors are with 100 estimators. The errors only decreased, so during hyperparameter\n",
    "tuning larger parameters should be tested."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Looking Back:\n",
    "* The best models (from lowest to highest scores): Logistic Regression, KNN, Extra Trees and XGBoost.\n",
    "* Decision Tree and AdaBoost models had the lowest performance, both below 80% accuracy.\n",
    ">\n",
    "> The pre-processing steps and baseline models have been analyzed.\n",
    "> The next step is to find the finest hyperparameters for the best models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tune Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors before tuning\n",
    "\n",
    "# log_reg_pred = cross_val_predict(log_reg, X_train_reduced, y_train, cv=6, verbose=2, n_jobs=-1)\n",
    "# joblib.dump(log_reg_pred, \"FMNIST/pred/ log_reg_pred.pkl\")\n",
    "log_reg_pred = joblib.load(r\"FMNIST/pred/ log_reg_pred.pkl\")\n",
    "display_evaluation(y_train, log_reg_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> * When the Logistic Regression parameter 'multi_class' is set to 'multinomial', it is using Softmax Regression\n",
    "which is optimized for multi-class inputs. Simply put, it estimates the probability that an instance belongs\n",
    "to each class and predicts the highest one. The sum of all probabilities is equal to 1.\n",
    "* Another approach: One-vs-One and One-vs-Rest wrapper methods.\n",
    ">\n",
    "> I'll search for the best hyperparameters, then try applying the best estimator with the wrapper methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning solver and penalty:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# both sag and saga solvers work well with large datasets and multiclass\n",
    "# they are a variation of gradient descent\n",
    "# convergence is faster when the data is scaled\n",
    "\n",
    "param_grid = [{\n",
    "    \"solver\": [\"sag\", \"saga\"]  # sag = stochastic average gradient, saga = variant of sag solver\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# log_reg = LogisticRegression(multi_class='multinomial', random_state=42)\n",
    "# log_reg_cv = GridSearchCV(log_reg, param_grid, cv=6, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "# log_reg_cv.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(log_reg_cv, \"FMNIST/scores/log_reg_cv.pkl\") # save local copy\n",
    "log_reg_cv = joblib.load(r\"FMNIST/scores/log_reg_cv.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "log_reg_cv.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "log_reg2 = log_reg_cv.best_estimator_\n",
    "log_reg2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = log_reg_cv.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"C\": [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]  # C controls regularization,\n",
    "    # light penalty if close to 1.0, strong penalty if close to 0.0\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# log_reg_cv2 = GridSearchCV(log_reg2, param_grid, cv=6, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "# log_reg_cv2.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(log_reg_cv2, \"FMNIST/scores/log_reg_cv2.pkl\")\n",
    "log_reg_cv2 = joblib.load(r\"FMNIST/scores/log_reg_cv2.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "log_reg_cv2.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "log_reg3 = log_reg_cv2.best_estimator_\n",
    "log_reg3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = log_reg_cv2.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### One-vs-Rest:\n",
    "> In this method, a binary classifier is trained for each class (10 in this case). The highest decision\n",
    "> score out of all the classifiers is the predicted class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ovr = OneVsRestClassifier(clone(log_reg3), n_jobs=-1)\n",
    "# ovr_scores = cross_val_score(ovr, X_train_reduced, y_train, scoring='accuracy', cv=6, n_jobs=-1)\n",
    "# joblib.dump(ovr_scores, \"FMNIST/scores/ovr_scores.pkl\")\n",
    "ovr_scores = joblib.load(r\"FMNIST/scores/ovr_scores.pkl\")\n",
    "display_scores(ovr_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### One-vs-One:\n",
    "> In this method, a binary classifier is trained for every pair of labels. Although more classifiers\n",
    "> are trained using this method, each needs to be trained only on the instances that represent the two\n",
    "> classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ovo = OneVsOneClassifier(clone(log_reg3), n_jobs=-1)\n",
    "# ovo_scores = cross_val_score(ovo, X_train_reduced, y_train, scoring='accuracy', cv=6, n_jobs=-1)\n",
    "# joblib.dump(ovo_scores, \"FMNIST/scores/ovo_scores.pkl\")\n",
    "ovo_scores = joblib.load(r\"FMNIST/scores/ovo_scores.pkl\")\n",
    "display_scores(ovo_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Final Logistic Regression model:** OvO Logistic Regression, using sag solver, l2 norm and 0.5 penalty."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors after tuning\n",
    "\n",
    "# log_reg_final_pred = cross_val_predict(ovo, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(log_reg_final_pred, \"FMNIST/pred/log_reg_final_pred.pkl\")\n",
    "log_reg_final_pred = joblib.load(r\"FMNIST/pred/log_reg_final_pred.pkl\")\n",
    "display_evaluation(y_train, log_reg_final_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tune KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors before tuning\n",
    "\n",
    "# knn_clf_pred = cross_val_predict(knn_clf, X_train_reduced, y_train, cv=6, verbose=2, n_jobs=-1)\n",
    "# joblib.dump(knn_clf_pred, \"FMNIST/pred/knn_clf_pred.pkl\")\n",
    "knn_clf_pred = joblib.load(r\"FMNIST/pred/knn_clf_pred.pkl\")\n",
    "display_evaluation(y_train, knn_clf_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> KNN does not output probabilities, which means that it can only be used for hard voting.\n",
    "In this case, One-vs-Rest, One-vs-One, and a 'soft' Voting Classifier can not be used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning weights and penalty:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"weights\": ['uniform', 'distance'],\n",
    "    \"p\": [1, 2]\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# knn_clf_cv = GridSearchCV(KNeighborsClassifier(), param_grid, cv=6, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "# knn_clf_cv.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(knn_clf_cv, \"FMNIST/scores/knn_clf_cv.pkl\")\n",
    "knn_clf_cv = joblib.load(r\"FMNIST/scores/knn_clf_cv.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "knn_clf_cv.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "knn_clf2 = knn_clf_cv.best_estimator_\n",
    "knn_clf2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = knn_clf_cv.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning number of neighbors:\n",
    ">> Note: There is an even number of classes. It is better to choose an odd number of neighbors to\n",
    ">> prevent ties."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 13, 15]\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# knn_clf_cv2 = GridSearchCV(knn_clf2, param_grid, cv=6, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "# knn_clf_cv2.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(knn_clf_cv2, \"FMNIST/scores/knn_clf_cv2.pkl\")\n",
    "knn_clf_cv2 = joblib.load(r\"FMNIST/scores/knn_clf_cv2.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "knn_clf_cv2.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "knn_clf3 = knn_clf_cv2.best_estimator_\n",
    "knn_clf3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = knn_clf_cv2.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Final KNN model:** Distance weights, l1 norm and 5 neighbors.\n",
    ">> The final KNN model has a slightly higher accuracy than the final Logistic Regression model!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors after tuning\n",
    "\n",
    "# knn_clf_final_pred = cross_val_predict(knn_clf3, X_train_reduced, y_train, cv=6, verbose=2, n_jobs=-1)\n",
    "## joblib.dump(knn_clf_final_pred, \"FMNIST/pred/knn_clf_final_pred.pkl\")\n",
    "knn_clf_final_pred = joblib.load(r\"FMNIST/pred/knn_clf_final_pred.pkl\")\n",
    "display_evaluation(y_train, knn_clf_final_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tune Extra Trees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors before tuning\n",
    "\n",
    "# ex_trees_pred = cross_val_predict(ex_trees, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(ex_trees_pred, \"FMNIST/pred/ex_trees_pred.pkl\")\n",
    "ex_trees_pred = joblib.load(r\"FMNIST/pred/ex_trees_pred.pkl\")\n",
    "display_evaluation(y_train, ex_trees_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning maximum number features per tree:\n",
    ">> Note: If this hyperparameter is set to 'None', it evaluates all features before a split. When dealing with high\n",
    "> dimensional data (such as image classification) it can be convenient to select a smaller amount\n",
    "> of features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"max_features\": ['sqrt', 0.2, 0.5, 0.8, None]  # max features that are evaluated before splitting a node\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ex_trees_cv = GridSearchCV(ExtraTreesClassifier(random_state=42, n_jobs=-1), param_grid, cv=6,\n",
    "#                         scoring='accuracy', verbose=2)\n",
    "# ex_trees_cv.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(ex_trees_cv, \"FMNIST/scores/ex_trees_cv.pkl\")\n",
    "ex_trees_cv = joblib.load(r\"FMNIST/scores/ex_trees_cv.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "ex_trees_cv.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "ex_trees2 = ex_trees_cv.best_estimator_\n",
    "ex_trees2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = ex_trees_cv.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning number of trees with Bagging and Pasting:\n",
    ">> Note: More estimators tend to improve accuracy at the expense of increasing computational time.\n",
    "> Additionally, from a certain amount of trees the model performance does not change much."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"n_estimators\": [50, 100, 200],  # number of trees in the forest\n",
    "    \"bootstrap\": [True, False]  # sampling with or without replacement\n",
    "    # True = bagging, False (sklearn default) = pasting\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ex_trees_cv2 = GridSearchCV(ex_trees2, param_grid, cv=6, scoring=\"accuracy\", verbose=2)\n",
    "# ex_trees_cv2.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(ex_trees_cv2, \"FMNIST/scores/ex_trees_cv2.pkl\")\n",
    "ex_trees_cv2 = joblib.load(r\"FMNIST/scores/ex_trees_cv2.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "ex_trees_cv2.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "ex_trees3 = ex_trees_cv2.best_estimator_\n",
    "ex_trees3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = ex_trees_cv2.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Final Extra Trees model:** 200 estimators, pasting method (sampling without replacement), and considers\n",
    "> all features when splitting a node."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors after tuning\n",
    "\n",
    "# ex_trees_final_pred = cross_val_predict(ex_trees3, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(ex_trees_final_pred, \"FMNIST/pred/ex_trees_final_pred.pkl\")\n",
    "ex_trees_final_pred = joblib.load(r\"FMNIST/pred/ex_trees_final_pred.pkl\")\n",
    "display_evaluation(y_train, ex_trees_final_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tune XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict accuracy and plot errors before tuning\n",
    "\n",
    "# xgb_clf_pred = cross_val_predict(xgb_clf, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(xgb_clf_pred, \"FMNIST/pred/xgb_clf_pred.pkl\")\n",
    "xgb_clf_pred = joblib.load(r\"FMNIST/pred/xgb_clf_pred.pkl\")\n",
    "display_evaluation(y_train, xgb_clf_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning the number of estimators:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 10,\n",
    "    \"eval_metric\": \"merror\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_scores_cv1 = cv(params, dmat_train, num_boost_round=200, nfold=6,\n",
    "#                    early_stopping_rounds=5, seed=42, metrics=\"merror\", verbose_eval=True)\n",
    "# joblib.dump(xgb_scores_cv1, \"FMNIST/scores/xgb_scores_cv1.pkl\")\n",
    "xgb_scores_cv1 = joblib.load(r\"FMNIST/scores/xgb_scores_cv1.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "curr_min = xgb_scores_cv1[\"test-merror-mean\"].argmin()\n",
    "curr_min"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot classification errors from cross-validation\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.lineplot(data=[xgb_scores_cv1[\"test-merror-mean\"], xgb_scores_cv1[\"train-merror-mean\"]])\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel(\"classification error rate\")\n",
    "plt.title(\"XGBoost Classification Errors\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best accuracy\n",
    "1 - xgb_scores_cv1[\"test-merror-mean\"][curr_min]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tuning learning rate and tree depth:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"max_depth\": [3, 4, 5, 6],  # maximum depth of a tree\n",
    "    \"learning_rate\": [0.1, 0.3]  # controls the weights after every boost (eta)\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_scores_cv2 = GridSearchCV(XGBClassifier(random_state=42, n_jobs=-1, eval_metric='merror', objective='multi:softmax',\n",
    "#                        num_class=10, use_label_encoder=False, n_estimators=122), param_grid, cv=6, scoring='accuracy', verbose=2)\n",
    "# xgb_scores_cv2.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(xgb_scores_cv2, \"FMNIST/scores/xgb_scores_cv2.pkl\")\n",
    "xgb_scores_cv2 = joblib.load(r\"FMNIST/scores/xgb_scores_cv2.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "xgb_scores_cv2.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "xgb_clf2 = xgb_scores_cv2.best_estimator_\n",
    "xgb_clf2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = xgb_scores_cv2.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"max_depth\": [6, 8],  # maximum depth of a tree\n",
    "    \"learning_rate\": [0.3, 0.5]  # controls the weights after every boost (eta)\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_scores_cv3 = GridSearchCV(xgb_clf2, param_grid, cv=6, scoring='accuracy', verbose=2)\n",
    "# xgb_scores_cv3.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(xgb_scores_cv3, \"FMNIST/scores/xgb_scores_cv3.pkl\")\n",
    "xgb_scores_cv3 = joblib.load(r\"FMNIST/scores/xgb_scores_cv3.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "xgb_scores_cv3.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "xgb_clf3 = xgb_scores_cv3.best_estimator_\n",
    "xgb_clf3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = xgb_scores_cv3.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    \"max_depth\": [8, 10],  # maximum depth of a tree\n",
    "    \"learning_rate\": [0.3]  # controls the weights after every boost (eta)\n",
    "}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# xgb_scores_cv4 = GridSearchCV(xgb_clf3, param_grid, cv=6, scoring='accuracy', verbose=2)\n",
    "# xgb_scores_cv4.fit(X_train_reduced, y_train)\n",
    "# joblib.dump(xgb_scores_cv4, \"FMNIST/scores/xgb_scores_cv4.pkl\")\n",
    "xgb_scores_cv4 = joblib.load(r\"FMNIST/scores/xgb_scores_cv4.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the best score\n",
    "xgb_scores_cv4.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best estimator\n",
    "xgb_clf4 = xgb_scores_cv4.best_estimator_\n",
    "xgb_clf4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show results for each iteration\n",
    "cvres = xgb_scores_cv4.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Final XGBoost model:** 0.3 learning rate, maximum tree depth 8."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# predict accuracy and plot errors after tuning\n",
    "\n",
    "# xgb_clf_final_pred = cross_val_predict(xgb_clf4, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(xgb_clf_final_pred, \"FMNIST/pred/xgb_clf_final_pred.pkl\")\n",
    "xgb_clf_final_pred = joblib.load(r\"FMNIST/pred/xgb_clf_final_pred.pkl\")\n",
    "display_evaluation(y_train, xgb_clf_final_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Voting Classifiers\n",
    "> The voting classifier is an ensemble that aggregates the predictions of multiple classifiers.\n",
    "> This often leads to higher prediction accuracy.\n",
    ">> Note: This does not guarantee that the voting classifier will achieve higher accuracy.\n",
    "> The classifiers were trained on the same data and might make similar errors, so it is preferable\n",
    "> to use classifiers with diverse algorithms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Hard Voting Classifier:\n",
    "> Predicts the class that received the most votes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "voting_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('log_reg', clone(log_reg3)),\n",
    "        ('knn', clone(knn_clf3)),\n",
    "        ('ex_trees', clone(ex_trees3))],\n",
    "    voting='hard')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# voting_hard_scores = cross_val_score(voting_hard, X_train_reduced, y_train, cv=6, scoring=\"accuracy\", verbose=2)\n",
    "# joblib.dump(voting_hard_scores, \"FMNIST/scores/voting_hard_scores.pkl\")\n",
    "voting_hard_scores = joblib.load(r\"FMNIST/scores/voting_hard_scores.pkl\")\n",
    "display_scores(voting_hard_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# voting_hard_pred = cross_val_predict(voting_hard, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(voting_hard_pred, \"FMNIST/pred/voting_hard_pred.pkl\")\n",
    "voting_hard_pred = joblib.load(r\"FMNIST/pred/voting_hard_pred.pkl\")\n",
    "display_evaluation(y_train, voting_hard_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Soft Voting Classifier:\n",
    "> Averages the predicted probabilities.\n",
    ">> Note: This voting method is only possible when all classifiers can output predicted probabilities."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "voting_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('log_reg', clone(log_reg3)),\n",
    "        ('ex_trees', clone(ex_trees3)),\n",
    "        ('xgb', clone(xgb_clf4))],\n",
    "    voting='soft')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# voting_soft_scores = cross_val_score(voting_soft, X_train_reduced, y_train, cv=6, scoring=\"accuracy\", verbose=2)\n",
    "# joblib.dump(voting_soft_scores, \"FMNIST/scores/voing_soft_scores.pkl\")\n",
    "voting_soft_scores = joblib.load(r\"FMNIST/scores/voing_soft_scores.pkl\")\n",
    "display_scores(voting_soft_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# voting_soft_pred = cross_val_predict(voting_soft, X_train_reduced, y_train, cv=6, verbose=2)\n",
    "# joblib.dump(voting_soft_pred, \"FMNIST/pred/voting_soft_pred.pkl\")\n",
    "voting_soft_pred = joblib.load(r\"FMNIST/pred/voting_soft_pred.pkl\")\n",
    "display_evaluation(y_train, voting_soft_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    "* Soft voting achieved a slightly higher accuracy compared to hard voting.\n",
    "* The final XGBoost model was still better than both voting classifiers.\n",
    "\n",
    "### Overall, the model with the highest accuracy score was XGBoost!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Last Attempt to Improve Accuracy With Data Augmentation:\n",
    ">\n",
    "> Data augmentation is a strategy that is used to create more data from the existing data.\n",
    "For example, the images could be rotated. Using this method can add more variation to the data.\n",
    ">\n",
    "> As a last attempt to improve the classification accuracy, I will increase the amount of training samples.\n",
    "For each image, I will create 4 new instances by shifting the images by one pixel in different\n",
    "directions (left, right, top, bottom). Lastly, I will fit the new training instances to the best\n",
    "model, XGBoost in this case, and evaluate the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using scipy multidimensional image processing shift function\n",
    "# function shifts array\n",
    "# fills the values beyond the edge with white pixels\n",
    "# x, y values for shifting according to axis\n",
    "def shift_image(image, x, y):\n",
    "    img = np.array(image).reshape(28, 28)\n",
    "    shifted_img = shift(img, [x, y], cval=0, mode=\"constant\")\n",
    "    return shifted_img.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# shift sample image by one pixel in each direction\n",
    "sample_img = X_train.iloc[5]\n",
    "img_shifted1 = shift_image(sample_img, -1, 0).reshape(28, 28)\n",
    "img_shifted2 = shift_image(sample_img, 1, 0).reshape(28, 28)\n",
    "img_shifted3 = shift_image(sample_img, 0, -1).reshape(28, 28)\n",
    "img_shifted4 = shift_image(sample_img, 0, 1).reshape(28, 28)\n",
    "\n",
    "# plot shifted images\n",
    "fig, ax = plt.subplots(1, 5, figsize=(21, 6))\n",
    "fig.suptitle(\"Image Augmentation Sample\", size=20)\n",
    "ax[0].imshow(np.array(sample_img).reshape(28, 28), cmap=\"binary\")\n",
    "ax[0].set_title(\"Original\", size=15)\n",
    "ax[1].imshow(img_shifted1, cmap=\"binary\")\n",
    "ax[1].set_title(\"Up\", size=15)\n",
    "ax[2].imshow(img_shifted2, cmap=\"binary\")\n",
    "ax[2].set_title(\"Down\", size=15)\n",
    "ax[3].imshow(img_shifted3, cmap=\"binary\")\n",
    "ax[3].set_title(\"Left\", size=15)\n",
    "ax[4].imshow(img_shifted4, cmap=\"binary\")\n",
    "ax[4].set_title(\"Right\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> As stated earlier, accuracy is sensitive to the test size. It would take a very long time to evaluate\n",
    "before and after augmentation with cross-validation on test sizes of 10,000 instances (30 folds after augmentation!).\n",
    "For this reason, I will create a validation set from the training instances and use it for evaluation.\n",
    ">\n",
    "> The validation set will include 10,000 instances, leaving 50,000 for training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and validation set stratified split\n",
    "X, X_val, y, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=10000, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make sure that class distribution is equal\n",
    "y.value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DataFrame shape (rows, columns)\n",
    "print(\"Training Set:\", X.shape)\n",
    "print(\"Validation Set:\", X_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### XGBoost Model Before Augmentation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_prep = full_pipeline.fit_transform(X)\n",
    "X_val_prep = full_pipeline.transform(X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_before = clone(xgb_clf4)\n",
    "# xgb_before.fit(X_prep, y)\n",
    "# joblib.dump(xgb_before, \"FMNIST/models/xgb_before.pkl\")\n",
    "xgb_before = joblib.load(r\"FMNIST/models/xgb_before.pkl\")\n",
    "# xgb_before_pred = xgb_before.predict(X_val_prep)\n",
    "# joblib.dump(xgb_before_pred, \"FMNIST/pred/xgb_before_pred.pkl\")\n",
    "xgb_before_pred = joblib.load(r\"FMNIST/pred/xgb_before_pred.pkl\")\n",
    "display_evaluation(y_val, xgb_before_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### XGBoost model after augmentation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# X_arr = np.array(X)\n",
    "# y_arr = np.array(y)\n",
    "\n",
    "# X_aug = [image for image in X_arr] # list containing training images arrays\n",
    "# y_aug = [label for label in y_arr] # list containing label values\n",
    "\n",
    "# for x, y in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "# for image, label in zip(X_arr, y_arr):\n",
    "# X_aug.append(shift_image(image, x, y))\n",
    "# y_aug.append(label)\n",
    "\n",
    "# convert data from list to numpy array\n",
    "# X_aug = np.array(X_aug)\n",
    "# y_aug = np.array(y_aug)\n",
    "\n",
    "# shuffle the training instances\n",
    "# shuffle_idx = np.random.permutation(len(X_aug))\n",
    "# X_aug = X_aug[shuffle_idx]\n",
    "# y_aug = y_aug[shuffle_idx]\n",
    "\n",
    "# joblib.dump(X_aug, \"FMNIST/data/X_aug.pkl\")\n",
    "# joblib.dump(y_aug, \"FMNIST/data/y_aug.pkl\")\n",
    "X_aug = joblib.load(r\"FMNIST/data/X_aug.pkl\")\n",
    "y_aug = joblib.load(r\"FMNIST/data/y_aug.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check augmented dataset shape\n",
    "X_aug.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check augmented dataset class distribution (should be equal)\n",
    "pd.Series(y_aug).value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use pipeline to transform data\n",
    "X_aug_prep = full_pipeline.fit_transform(X_aug, y_aug)\n",
    "X_val_aug_prep = full_pipeline.transform(X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_after = clone(xgb_clf4)\n",
    "# xgb_after.fit(X_aug_prep, y_aug)\n",
    "# joblib.dump(xgb_after, \"FMNIST/models/xgb_after.pkl\")\n",
    "xgb_after = joblib.load(r\"FMNIST/models/xgb_after.pkl\")\n",
    "# xgb_after_pred = xgb_after.predict(X_val_aug_prep)\n",
    "# joblib.dump(xgb_after_pred, \"FMNIST/pred/xgb_after_pred.pkl\")\n",
    "xgb_after_pred = joblib.load(r\"FMNIST/pred/xgb_after_pred.pkl\")\n",
    "display_evaluation(y_val, xgb_after_pred, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Conclusion:\n",
    "> **Data augmentation improved the accuracy score of the final model!**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Fit model on whole augmented training set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_arr = np.array(X_train)\n",
    "# y_arr = np.array(y_train)\n",
    "\n",
    "# X_train_augmented = [image for image in X_arr] # list containing training images arrays\n",
    "# y_train_augmented = [label for label in y_arr] # list containing label values\n",
    "\n",
    "# for x, y in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "# for image, label in zip(X_arr, y_arr):\n",
    "# X_train_augmented.append(shift_image(image, x, y))\n",
    "# y_train_augmented.append(label)\n",
    "\n",
    "# convert data from list to numpy array\n",
    "# X_train_augmented = np.array(X_train_augmented)\n",
    "# y_train_augmented = np.array(y_train_augmented)\n",
    "\n",
    "# shuffle the training instances\n",
    "# shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
    "# X_train_augmented = X_train_augmented[shuffle_idx]\n",
    "# y_train_augmented = y_train_augmented[shuffle_idx]\n",
    "\n",
    "# joblib.dump(X_train_augmented, \"FMNIST/data/X_train_augmented.pkl\")\n",
    "# joblib.dump(y_train_augmented, \"FMNIST/data/y_train_augmented.pkl\")\n",
    "X_train_augmented = joblib.load(r\"FMNIST/data/X_train_augmented.pkl\")\n",
    "y_train_augmented = joblib.load(r\"FMNIST/data/y_train_augmented.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check augmented dataset shape\n",
    "X_train_augmented.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check augmented dataset class distribution (should be equal)\n",
    "pd.Series(y_train_augmented).value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# use pipeline to transform data\n",
    "X_train_aug_prepared = full_pipeline.fit_transform(X_train_augmented)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# xgb_clf_final = clone(xgb_clf4)\n",
    "# xgb_clf_final.fit(X_train_aug_prepared, y_train_augmented)\n",
    "# joblib.dump(xgb_clf_final, \"FMNIST/models/xgb_clf_final.pkl\")\n",
    "xgb_clf_final = joblib.load(r\"FMNIST/models/xgb_clf_final.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Evaluating the Test Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# separate test set predictors and labels\n",
    "X_test = test_set.drop(\"label\", axis=1)\n",
    "y_test = test_set[\"label\"].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform test set\n",
    "X_test_prepared = full_pipeline.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate test set predictions\n",
    "final_predictions = xgb_clf_final.predict(X_test_prepared)\n",
    "display_evaluation(y_test, final_predictions, True, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Resources:\n",
    "1. Fashion MNIST Dataset <a href=\"https://www.kaggle.com/zalando-research/fashionmnist\"\n",
    "> title=\"Kaggle\">link</a>\n",
    "2. Feature Scaling Article <a href=\"https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35\"\n",
    "> title=\"towardsdatascience\">link</a>\n",
    "3. PCA Article <a href=\"https://towardsdatascience.com/pca-is-not-feature-selection-3344fb764ae6\"\n",
    "> title=\"towardsdatascience\">link</a>\n",
    "4. Extra Trees and Random Forest Comparison <a href=\"https://quantdare.com/what-is-the-difference-between-extra-trees-and-random-forest/\"\n",
    "> title=\"quantdare\">link</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-bcecade4",
   "language": "python",
   "display_name": "PyCharm (1SB-Final-Project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}